training_data$delivery_date[training_data$delivery_time < 0] = NA
test_data$delivery_date[test_data$delivery_time < 0] = NA
training_data$delivery_time[training_data$delivery_time < 0] = NA
test_data$delivery_time[test_data$delivery_time < 0] = NA
hist(training_data$delivery_time)
table(training_data$delivery_time, useNA="ifany")
boxplot(training_data$delivery_time ~ training_data$return_shipment)
# Manual discretization of "delivery time"
training_data$delivery_time_discret = factor(rep("NA", nrow(training_data)), levels=c("NA", "<= 5d", "> 5d"))
test_data$delivery_time_discret = factor(rep("NA", nrow(test_data)), levels=c("NA", "<= 5d", "> 5d"))
training_data$delivery_time_discret[training_data$delivery_time <= 5] = "<= 5d"
test_data$delivery_time_discret[test_data$delivery_time <= 5] = "<= 5d"
training_data$delivery_time_discret[training_data$delivery_time > 5] = "> 5d"
test_data$delivery_time_discret[test_data$delivery_time > 5] = "> 5d"
str(training_data)
setwd("~/Desktop/DMC2")
# Business Analytics
# Data Mining Cup Introduction
#
# Please note, that this script only has the nature of proposal. It provides useful functions for the steps of data mining but does not cover all possibilities.
# The caret package is used (http://topepo.github.io/caret/index.html)
#install.packages("caret")
library(caret)
library(plyr)
# For reasons of traceability you must use a fixed seed
set.seed(42) # do NOT CHANGE this seed
######################################################
# 2. Load
training_data = read.csv("training.csv", sep=",")
test_data = read.csv("test.csv", sep=",")
######################################################
# 3. Data Preparation
# (using both training and test data)
# do NOT DELETE any instances in the test data
# Rename columns
names(training_data)[names(training_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(test_data)[names(test_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(training_data)[names(training_data) == 'WorkZOne'] <- 'WorkZone'
names(test_data)[names(test_data) == 'WorkZOne'] <- 'WorkZone'
# Nominal attributes
setLevels <- function (data, attr, levels) {
data[,attr] <- factor(data[,attr], levels=levels)
return(data)
}
# Yes/ No fields
levels <- c("No", "Yes")
# training data
training_data <- setLevels(training_data, "Accident", levels)
training_data <- setLevels(training_data, "Belts", levels)
training_data <- setLevels(training_data, "PersonalInjury", levels)
training_data <- setLevels(training_data, "PropertyDamage", levels)
training_data <- setLevels(training_data, "Fatal", levels)
training_data <- setLevels(training_data, "CommercialLicense", levels)
training_data <- setLevels(training_data, "HAZMAT", levels)
training_data <- setLevels(training_data, "CommercialVehicle", levels)
training_data <- setLevels(training_data, "Alcohol", levels)
training_data <- setLevels(training_data, "WorkZone", levels)
training_data <- setLevels(training_data, "ContributedToAccident", levels)
# test data
test_data <- setLevels(test_data, "Accident", levels)
test_data <- setLevels(test_data, "Belts", levels)
test_data <- setLevels(test_data, "PersonalInjury", levels)
test_data <- setLevels(test_data, "PropertyDamage", levels)
test_data <- setLevels(test_data, "Fatal", levels)
test_data <- setLevels(test_data, "CommercialLicense", levels)
test_data <- setLevels(test_data, "HAZMAT", levels)
test_data <- setLevels(test_data, "CommercialVehicle", levels)
test_data <- setLevels(test_data, "Alcohol", levels)
test_data <- setLevels(test_data, "WorkZone", levels)
test_data <- setLevels(test_data, "ContributedToAccident", levels)
# Remove fields that have the very same value (Feature selection)
drop <- c("TimeOfStop", "Agency", "SubAgency", "Geolocation", "Article", "Location")
training_data <- training_data[,!(names(training_data) %in% drop)]
test_data <- test_data[,!(names(test_data) %in% drop)]
str(training_Data)
str(training_d\ata)
str(training_data)
hist(training_data$Latitude)
max(training_data$Latitude)
min(training_data$Latitude)
min(training_data$Longitude)
maximum(training_data$Longitude)
?max
max(training_data$Longitude, na.rm = TRUE)
min(training_data$Longitude, na.rm = TRUE)
min(training_data$Latitude, na.rm = TRUE)
max(training_data$Latitude, na.rm = TRUE)
is.na(training_data)
colSums(is.na(training_data))
colSums(is.nan(training_data))
colSums(is.na(training_data))
colSums(is.na(test_data))
colSums(is.na(training_data))
na_col = colnames(training_data[sapply(training_data,is.numeric)])
na_col
colnames(training_data[sapply(training_data,is.numeric)])
is.character(colnames(training_data[sapply(training_data,is.numeric)]))
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[na_col])
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
itude
training_data$Longitude = preprocessed$Longit
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
trainig_data$Longitude
training_data$Longitude
training_data = read.csv("training.csv", sep=",")
test_data = read.csv("test.csv", sep=",")
######################################################
# 3. Data Preparation
# (using both training and test data)
# do NOT DELETE any instances in the test data
# Rename columns
names(training_data)[names(training_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(test_data)[names(test_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(training_data)[names(training_data) == 'WorkZOne'] <- 'WorkZone'
names(test_data)[names(test_data) == 'WorkZOne'] <- 'WorkZone'
# Nominal attributes
setLevels <- function (data, attr, levels) {
data[,attr] <- factor(data[,attr], levels=levels)
return(data)
}
# Yes/ No fields
levels <- c("No", "Yes")
# training data
training_data <- setLevels(training_data, "Accident", levels)
training_data <- setLevels(training_data, "Belts", levels)
training_data <- setLevels(training_data, "PersonalInjury", levels)
training_data <- setLevels(training_data, "PropertyDamage", levels)
training_data <- setLevels(training_data, "Fatal", levels)
training_data <- setLevels(training_data, "CommercialLicense", levels)
training_data <- setLevels(training_data, "HAZMAT", levels)
training_data <- setLevels(training_data, "CommercialVehicle", levels)
training_data <- setLevels(training_data, "Alcohol", levels)
training_data <- setLevels(training_data, "WorkZone", levels)
training_data <- setLevels(training_data, "ContributedToAccident", levels)
# test data
test_data <- setLevels(test_data, "Accident", levels)
test_data <- setLevels(test_data, "Belts", levels)
test_data <- setLevels(test_data, "PersonalInjury", levels)
test_data <- setLevels(test_data, "PropertyDamage", levels)
test_data <- setLevels(test_data, "Fatal", levels)
test_data <- setLevels(test_data, "CommercialLicense", levels)
test_data <- setLevels(test_data, "HAZMAT", levels)
test_data <- setLevels(test_data, "CommercialVehicle", levels)
test_data <- setLevels(test_data, "Alcohol", levels)
test_data <- setLevels(test_data, "WorkZone", levels)
test_data <- setLevels(test_data, "ContributedToAccident", levels)
training_data$Latitude
colSums(is.na(training_data))
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
colSums(is.na(training_data))
?omit.na
na.omit?
s
?na.omit
# remove NA Year and NA color
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
# remove other NA values
na.omit(training_data)
colSums(is.na(training_data))
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
# remove other NA values
training_data <- training_data[complete.cases(training_data),]
colSums(is.na(training_data))
length(training_data)
View(training_Data)
View(training_data)
training_data
# Business Analytics
# Data Mining Cup Introduction
#
# Please note, that this script only has the nature of proposal. It provides useful functions for the steps of data mining but does not cover all possibilities.
# The caret package is used (http://topepo.github.io/caret/index.html)
#install.packages("caret")
library(caret)
library(plyr)
# For reasons of traceability you must use a fixed seed
set.seed(42) # do NOT CHANGE this seed
######################################################
# 2. Load
training_data = read.csv("training.csv", sep=",")
test_data = read.csv("test.csv", sep=",")
######################################################
# 3. Data Preparation
# (using both training and test data)
# do NOT DELETE any instances in the test data
# Rename columns
names(training_data)[names(training_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(test_data)[names(test_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(training_data)[names(training_data) == 'WorkZOne'] <- 'WorkZone'
names(test_data)[names(test_data) == 'WorkZOne'] <- 'WorkZone'
# Nominal attributes
setLevels <- function (data, attr, levels) {
data[,attr] <- factor(data[,attr], levels=levels)
return(data)
}
# Yes/ No fields
levels <- c("No", "Yes")
# training data
training_data <- setLevels(training_data, "Accident", levels)
training_data <- setLevels(training_data, "Belts", levels)
training_data <- setLevels(training_data, "PersonalInjury", levels)
training_data <- setLevels(training_data, "PropertyDamage", levels)
training_data <- setLevels(training_data, "Fatal", levels)
training_data <- setLevels(training_data, "CommercialLicense", levels)
training_data <- setLevels(training_data, "HAZMAT", levels)
training_data <- setLevels(training_data, "CommercialVehicle", levels)
training_data <- setLevels(training_data, "Alcohol", levels)
training_data <- setLevels(training_data, "WorkZone", levels)
training_data <- setLevels(training_data, "ContributedToAccident", levels)
# test data
test_data <- setLevels(test_data, "Accident", levels)
test_data <- setLevels(test_data, "Belts", levels)
test_data <- setLevels(test_data, "PersonalInjury", levels)
test_data <- setLevels(test_data, "PropertyDamage", levels)
test_data <- setLevels(test_data, "Fatal", levels)
test_data <- setLevels(test_data, "CommercialLicense", levels)
test_data <- setLevels(test_data, "HAZMAT", levels)
test_data <- setLevels(test_data, "CommercialVehicle", levels)
test_data <- setLevels(test_data, "Alcohol", levels)
test_data <- setLevels(test_data, "WorkZone", levels)
test_data <- setLevels(test_data, "ContributedToAccident", levels)
# Handle NA values
# Only for longitude and latitude, replace with median
# remove NA Year and NA color
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
# remove other NA values
training_data <- training_data[complete.cases(training_data),]
colSums(is.na(training_data))
# Remove fields that have the very same value (Feature selection)
drop <- c("TimeOfStop", "Agency", "SubAgency", "Geolocation", "Article", "Location")
training_data <- training_data[,!(names(training_data) %in% drop)]
test_data <- test_data[,!(names(test_data) %in% drop)]
str(training_data)
training_data
?complete.cases
complete.cases(training_data)
# Business Analytics
# Data Mining Cup Introduction
#
# Please note, that this script only has the nature of proposal. It provides useful functions for the steps of data mining but does not cover all possibilities.
# The caret package is used (http://topepo.github.io/caret/index.html)
#install.packages("caret")
library(caret)
library(plyr)
# For reasons of traceability you must use a fixed seed
set.seed(42) # do NOT CHANGE this seed
######################################################
# 2. Load
training_data = read.csv("training.csv", sep=",")
test_data = read.csv("test.csv", sep=",")
######################################################
# 3. Data Preparation
# (using both training and test data)
# do NOT DELETE any instances in the test data
# Rename columns
names(training_data)[names(training_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(test_data)[names(test_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(training_data)[names(training_data) == 'WorkZOne'] <- 'WorkZone'
names(test_data)[names(test_data) == 'WorkZOne'] <- 'WorkZone'
# Nominal attributes
setLevels <- function (data, attr, levels) {
data[,attr] <- factor(data[,attr], levels=levels)
return(data)
}
# Yes/ No fields
levels <- c("No", "Yes")
# training data
training_data <- setLevels(training_data, "Accident", levels)
training_data <- setLevels(training_data, "Belts", levels)
training_data <- setLevels(training_data, "PersonalInjury", levels)
training_data <- setLevels(training_data, "PropertyDamage", levels)
training_data <- setLevels(training_data, "Fatal", levels)
training_data <- setLevels(training_data, "CommercialLicense", levels)
training_data <- setLevels(training_data, "HAZMAT", levels)
training_data <- setLevels(training_data, "CommercialVehicle", levels)
training_data <- setLevels(training_data, "Alcohol", levels)
training_data <- setLevels(training_data, "WorkZone", levels)
training_data <- setLevels(training_data, "ContributedToAccident", levels)
# test data
test_data <- setLevels(test_data, "Accident", levels)
test_data <- setLevels(test_data, "Belts", levels)
test_data <- setLevels(test_data, "PersonalInjury", levels)
test_data <- setLevels(test_data, "PropertyDamage", levels)
test_data <- setLevels(test_data, "Fatal", levels)
test_data <- setLevels(test_data, "CommercialLicense", levels)
test_data <- setLevels(test_data, "HAZMAT", levels)
test_data <- setLevels(test_data, "CommercialVehicle", levels)
test_data <- setLevels(test_data, "Alcohol", levels)
test_data <- setLevels(test_data, "WorkZone", levels)
test_data <- setLevels(test_data, "ContributedToAccident", levels)
# Handle NA values
# Only for longitude and latitude, replace with median
# remove NA Year and NA color
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
complete.cases(training_data)
str(training_data)
# Remove fields that have the very same value (Feature selection)
drop <- c("TimeOfStop", "Agency", "SubAgency", "Geolocation", "Article", "Location")
training_data <- training_data[,!(names(training_data) %in% drop)]
test_data <- test_data[,!(names(test_data) %in% drop)]
# Handle NA values
# Only for longitude and latitude, replace with median
# remove NA Year and NA color
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
colSums(is.na(preprocessed))
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
# remove other NA values
training_data <- training_data[complete.cases(training_data),]
colSums(is.na(training_data))
View(training_data)
table(training_data$Citation)
str(training_data)
install.packages("FSelector")
library(FSelector)
weights_info_gain = information.gain(Citation ~ ., data=training_data)
weights_info_gain
weights_gain_ratio = gain.ratio(Citation ~ ., data=training_data)
weights_gain_ratio
max(training_data$Year)
min(training_data$Year)
length(training_data[training_data$Year == 0, ])
training_data$Year[training_data$Year == 2077] = 2007
max(training_data$Year)
training_data$Year[training_data$Year == 0] = median(training_data$Year)
min(training_data$Year)
unique(training_data$Year)
str(training_data)
install.packages("arules")
library(arules)
library(arules)
# Equal frequency binning
equiFreqLatitude = discretize(training_data$Latitude, categories=15, method="frequency", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr, useNA="ifany")
?discretize
equiFreqLatitude = discretize(training_data$Latitude, categories=14, method="interval", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr, useNA="ifany")
equiFreqLatitude = discretize(training_data$Latitude, categories=14, method="cluster", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr, useNA="ifany")
str(training_data)
equiFreqLatitude = discretize(training_data$Latitude, categories=10, method="cluster", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr, useNA="ifany")
equiFreqLatitude = discretize(training_data$Latitude, categories=5, method="cluster", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr, useNA="ifany")
equiFreqLatitude = discretize(training_data$Latitude, categories=10, method="cluster", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr, useNA="ifany")
equiFreqLongitude = discretize(training_data$Longitude, categories=10, method="cluster", onlycuts=TRUE)
training_data$LongitudeDiscr = cut(training_data$Longitude, breaks=equiFreqLongitude, ordered_result=TRUE, right=FALSE)
test_data$LongitudeDiscr = cut(test_data$Longitude, breaks=equiFreqLongitude, ordered_result=TRUE, right=FALSE)
table(training_data$LongitudeDiscr, useNA="ifany")
equiFreqLongitude = discretize(training_data$Longitude, categories=9, method="cluster", onlycuts=TRUE)
training_data$LongitudeDiscr = cut(training_data$Longitude, breaks=equiFreqLongitude, ordered_result=TRUE, right=FALSE)
test_data$LongitudeDiscr = cut(test_data$Longitude, breaks=equiFreqLongitude, ordered_result=TRUE, right=FALSE)
table(training_data$LongitudeDiscr, useNA="ifany")
is.na(training_data$Longitude)
str(training_data)
equiFreqLatitude = discretize(training_data$Latitude, categories=9, method="cluster", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
table(training_data$LatitudeDiscr)
# Business Analytics
# Data Mining Cup Introduction
#
# Please note, that this script only has the nature of proposal. It provides useful functions for the steps of data mining but does not cover all possibilities.
# The caret package is used (http://topepo.github.io/caret/index.html)
#install.packages("caret")
library(caret)
library(plyr)
# For reasons of traceability you must use a fixed seed
set.seed(42) # do NOT CHANGE this seed
######################################################
# 2. Load
training_data = read.csv("training.csv", sep=",")
test_data = read.csv("test.csv", sep=",")
######################################################
# 3. Data Preparation
# (using both training and test data)
# do NOT DELETE any instances in the test data
# Rename columns
names(training_data)[names(training_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(test_data)[names(test_data) == 'ProperyDamage'] <- 'PropertyDamage'
names(training_data)[names(training_data) == 'WorkZOne'] <- 'WorkZone'
names(test_data)[names(test_data) == 'WorkZOne'] <- 'WorkZone'
# Nominal attributes
setLevels <- function (data, attr, levels) {
data[,attr] <- factor(data[,attr], levels=levels)
return(data)
}
# Unify Yes/ No fields
# --------------------------------------------------
levels <- c("No", "Yes")
# training data
training_data <- setLevels(training_data, "Accident", levels)
training_data <- setLevels(training_data, "Belts", levels)
training_data <- setLevels(training_data, "PersonalInjury", levels)
training_data <- setLevels(training_data, "PropertyDamage", levels)
training_data <- setLevels(training_data, "Fatal", levels)
training_data <- setLevels(training_data, "CommercialLicense", levels)
training_data <- setLevels(training_data, "HAZMAT", levels)
training_data <- setLevels(training_data, "CommercialVehicle", levels)
training_data <- setLevels(training_data, "Alcohol", levels)
training_data <- setLevels(training_data, "WorkZone", levels)
training_data <- setLevels(training_data, "ContributedToAccident", levels)
# test data
test_data <- setLevels(test_data, "Accident", levels)
test_data <- setLevels(test_data, "Belts", levels)
test_data <- setLevels(test_data, "PersonalInjury", levels)
test_data <- setLevels(test_data, "PropertyDamage", levels)
test_data <- setLevels(test_data, "Fatal", levels)
test_data <- setLevels(test_data, "CommercialLicense", levels)
test_data <- setLevels(test_data, "HAZMAT", levels)
test_data <- setLevels(test_data, "CommercialVehicle", levels)
test_data <- setLevels(test_data, "Alcohol", levels)
test_data <- setLevels(test_data, "WorkZone", levels)
test_data <- setLevels(test_data, "ContributedToAccident", levels)
# Remove missing features
# ----------------------------------------------------
drop <- c("TimeOfStop", "Agency", "SubAgency", "Geolocation", "Article", "Location")
training_data <- training_data[,!(names(training_data) %in% drop)]
test_data <- test_data[,!(names(test_data) %in% drop)]
# Handle NA values
# ----------------------------------------------------
# Only for longitude and latitude, replace with median
# remove NA Year and NA color
naCols = c("Longitude", "Latitude")
pp<- preProcess(training_data[naCols], method = c("medianImpute"))
preprocessed <- predict(pp, newdata = training_data[naCols])
training_data$Latitude = preprocessed$Latitude
training_data$Longitude = preprocessed$Longitude
# remove other NA values
training_data <- training_data[complete.cases(training_data),]
colSums(is.na(training_data))
# Fix errors
# ---------------------------------------------------
training_data$Year[training_data$Year == 2077] = 2007
training_data$Year[training_data$Year == 0] = median(training_data$Year)
# Discretize Longitude and Latitude
# ----------------------------------------------------
# So far I've seen no obvious "separation" in the locations, so
# discretization with a single rule seems meaningless
# install.packages("arules")
library(arules)
# Equal frequency binning
equiFreqLatitude = discretize(training_data$Latitude, categories=9, method="cluster", onlycuts=TRUE)
training_data$LatitudeDiscr = cut(training_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
test_data$LatitudeDiscr = cut(test_data$Latitude, breaks=equiFreqLatitude, ordered_result=TRUE, right=FALSE)
# table(training_data$LatitudeDiscr, useNA="ifany")
# str(training_data)
equiFreqLongitude = discretize(training_data$Longitude, categories=9, method="cluster", onlycuts=TRUE)
training_data$LongitudeDiscr = cut(training_data$Longitude, breaks=equiFreqLongitude, ordered_result=TRUE, right=FALSE)
test_data$LongitudeDiscr = cut(test_data$Longitude, breaks=equiFreqLongitude, ordered_result=TRUE, right=FALSE)
# table(training_data$LongitudeDiscr, useNA="ifany")
# str(training_data)
# Multicolinearity
# ----------------------------------------------------
# * Check columns
# * remove ones that are collinear
# Feature selection
# ----------------------------------------------------
#install.packages("FSelector")
library(FSelector)
# Calculate weights for the attributes using Info Gain and Gain Ratio
weights_info_gain = information.gain(Citation ~ ., data=training_data)
weights_info_gain
weights_gain_ratio = gain.ratio(Citation ~ ., data=training_data)
weights_gain_ratio
str(training_data)
unique(training_data$Race)
str(test_data)
